\relax 
\@writefile{toc}{\contentsline {title}{Massive Classification From Eigenvector: Comparing Neural Network, Decision Tree \& Maximum Likelihood}{1}\protected@file@percent }
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Hongxiang Zhang}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data Processing}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the process that extracts the input eigenvectors. The first block indicates the input image, then passes it to the second block which is ResNet to extract features. The output is eigenvectors.}}{2}\protected@file@percent }
\newlabel{fig1}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural Network}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The neural network architecture considered in this paper. n1 stands for the number of neurons in FC1 which is the number of eigenvectors, n3 stands for the numbers of neurons in softmax layer which is the number of classification}}{3}\protected@file@percent }
\newlabel{fig2}{{2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{Fully connected layer}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Activation Function}{3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Three graph represent the activation function graph corresponding to sigmoid, Relu and LeakyRelu respectively}}{4}\protected@file@percent }
\newlabel{fig3}{{3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{Softmax layer}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of the softmax calculation process. After calculation, each input is map to range$[0,1]$.}}{4}\protected@file@percent }
\newlabel{fig4}{{4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Decision Tree}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Maximum Likelihood}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results and Discussion}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Neural Network}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Network Architectural}{5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparesion between 1,2 and 3 layer network}}{5}\protected@file@percent }
\newlabel{tab1}{{1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Three graph shows the curve of loss to epoch corresponding to one layer, two-layer, and three-layer network. All models run with the same dataset and same epochs.}}{6}\protected@file@percent }
\newlabel{fig5}{{5}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Hyperparameters}{6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparesion between Learning rate, epoches and optimiser}}{6}\protected@file@percent }
\newlabel{tab2}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Three graph shows the curve of loss to epoch corresponding to 0.01 learning rate, 0.005 learning rate, 0.001 learning rate two-layer network. All models run with the same dataset and same epochs.}}{7}\protected@file@percent }
\newlabel{fig6}{{6}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Dropout layer}{7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparesion between different dropout}}{7}\protected@file@percent }
\newlabel{tab3}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Decision Tree}{7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Decision Tree Performance }}{7}\protected@file@percent }
\newlabel{tab4}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Maximum Likelihood}{8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Maximum Likelihood Performance }}{8}\protected@file@percent }
\newlabel{tab5}{{5}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Summary and Discussion}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion and Future Work}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Reference}{8}\protected@file@percent }
\newlabel{LastPage}{{}{9}}
\xdef\lastpage@lastpage{9}
\gdef\lastpage@lastpageHy{}
